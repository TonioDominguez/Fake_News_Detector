{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1baf7525-2822-4f85-969c-3a9595eb1859",
   "metadata": {},
   "source": [
    "<h1 style=\"color: #492c68;\">01 | USER INPUT FAKE NEWS DETECTOR</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bdc7630-4b1e-4a44-8838-562a38036ccd",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h2 style=\"color: #327a81;\">Libraries</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41c691ae-b11e-459b-ad1a-f34df6b011eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Basic libraries\n",
    "import pandas as pd # data manipulation\n",
    "import numpy as np # funciones matemáticas\n",
    "\n",
    "## EDA Libraries \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "## Settings\n",
    "pd.set_option('display.max_columns', None) # display all columns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') # ignore warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36c1e4c3-56c0-49c9-b9ff-1d1d4a409390",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\antdo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "## Pre-processing for modeling\n",
    "\n",
    "import re # Natural Language Toolkit to adapt text lines for ML\n",
    "import nltk\n",
    "nltk.download('stopwords') #language package for spanish\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "import gensim # take text for clean and tokenize list of words\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from gensim import corpora\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af27ce88-1932-475f-b86b-79d704c41c27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Importing Naive Bayes model to predict Fake News\n",
    "\n",
    "with open(\"model_NB.pkl\", \"rb\") as file:\n",
    "    model_NB = pickle.load(file)\n",
    "    \n",
    "with open(\"vectorizer.pkl\", \"rb\") as file:\n",
    "    vectorizer = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3591f2b8-dbcf-41be-82d6-13702eb060e5",
   "metadata": {},
   "source": [
    "<h2 style=\"color: #327a81;\">Input Detector Creation</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54b07af1-67e2-4e01-a1b4-20ee3fb1e965",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Need to clean \"text\" to provide value words for ML modeling\n",
    "# NLTK will help us to earse unnecesary prepositions and others\n",
    "\n",
    "stop_words = list(stopwords.words(\"spanish\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da360ed7-093a-4bf3-8994-4a8cda55c6f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stop_words.extend([\"según\", \"ahora\", \"después\", \"todas\", \"toda\", \"todo\", \"todos\", \"sólo\", \"solo\", \"sido\", \"están\",\n",
    "                   \"estan\", \"hacer\", \"hecho\", \"puede\", \"tras\", \"cabe\", \"bajo\", \"durante\", \"mediante\", \"cada\", \"me\",\n",
    "                  \"lunes\", \"martes\", \"miércoles\", \"jueves\", \"viernes\", \"sabado\", \"domingo\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7173e26f-a855-4b1a-b509-76f3f7eb4167",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def text_cleaner(text):\n",
    "    words = []\n",
    "    new_text = []\n",
    "    \n",
    "    for word in gensim.utils.simple_preprocess(text):\n",
    "        if word not in gensim.parsing.preprocessing.STOPWORDS and word not in stop_words and len(word) > 4:\n",
    "            words.append(word)\n",
    "    new_text = \" \".join(words)\n",
    "    \n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4650f1f-bf71-42cd-aa9c-5151ea94b7ff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Introduce la noticia:  Denuncian que Pablo Iglesias ha robado imágenes de comida para la carta de su bar\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Esta noticia es FALSA.\n"
     ]
    }
   ],
   "source": [
    "text = input(\"Introduce la noticia: \")\n",
    "new_text = text_cleaner(text)\n",
    "\n",
    "df = pd.DataFrame({\"text\": [new_text]})\n",
    "text_ml = df[\"text\"]\n",
    "text_vect = vectorizer.transform(text_ml)\n",
    "text_prediction = model_NB.predict(text_vect)\n",
    "\n",
    "if text_prediction == 0:\n",
    "    print(\"\")\n",
    "    print(\"Esta noticia es FALSA.\")\n",
    "elif text_prediction == 1:\n",
    "    print(\"\")\n",
    "    print(\"Esta noticia es VERDADERA.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
